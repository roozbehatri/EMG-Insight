{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28613af1-ca92-48b1-acba-fd3c6dc5c4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT = Path(\"..\")\n",
    "DATA_PROCESSED = ROOT / \"data\" / \"processed\" / \"db1\"\n",
    "MODELS_DIR     = ROOT / \"models\" / \"db1\"\n",
    "REPORTS_DIR    = ROOT / \"reports\" / \"db1\"\n",
    "for p in [MODELS_DIR, REPORTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58cc9a2b-0a90-45dc-a10e-30f1a7b92621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (312643, 50, 10) Val: (93436, 50, 10) Test: (93476, 50, 10) classes: 23\n",
      "train classes: 23 min/max per class: 400 400\n",
      "val classes: 23 min/max per class: 100 100\n",
      "test classes: 23 min/max per class: 100 100\n",
      "Train: (9200, 50, 10) Val: (2300, 50, 10) Test: (2300, 50, 10) classes: 23\n"
     ]
    }
   ],
   "source": [
    "# --- Load as before ---\n",
    "train_npz = DATA_PROCESSED / \"dl_train_db1_raw.npz\"\n",
    "val_npz   = DATA_PROCESSED / \"dl_val_db1_raw.npz\"\n",
    "test_npz  = DATA_PROCESSED / \"dl_test_db1_raw.npz\"\n",
    "\n",
    "ztr = np.load(train_npz, allow_pickle=True)\n",
    "zva = np.load(val_npz,   allow_pickle=True)\n",
    "zte = np.load(test_npz,  allow_pickle=True)\n",
    "\n",
    "Xtr, ytr = ztr[\"X\"], ztr[\"y\"]\n",
    "Xva, yva = zva[\"X\"], zva[\"y\"]\n",
    "Xte, yte = zte[\"X\"], zte[\"y\"]\n",
    "idx_to_label = ztr[\"idx_to_label\"]\n",
    "n_classes = len(idx_to_label)\n",
    "print(\"Train:\", Xtr.shape, \"Val:\", Xva.shape, \"Test:\", Xte.shape, \"classes:\", n_classes)\n",
    "\n",
    "# # --- Subsample for CPU demo ---\n",
    "# # choose a fraction or a fixed number per split\n",
    "# N_train, N_val, N_test = 2000, 500, 500   # adjust as needed\n",
    "\n",
    "# rng = np.random.default_rng(seed=0)  # reproducible\n",
    "# train_idx = rng.choice(len(Xtr), size=min(N_train, len(Xtr)), replace=False)\n",
    "# val_idx   = rng.choice(len(Xva), size=min(N_val, len(Xva)), replace=False)\n",
    "# test_idx  = rng.choice(len(Xte), size=min(N_test, len(Xte)), replace=False)\n",
    "\n",
    "# Xtr, ytr = Xtr[train_idx], ytr[train_idx]\n",
    "# Xva, yva = Xva[val_idx], yva[val_idx]\n",
    "# Xte, yte = Xte[test_idx], yte[test_idx]\n",
    "\n",
    "def stratified_cap(X, y, per_class=200, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    keep_idx = []\n",
    "    for c in np.unique(y):\n",
    "        idx = np.where(y == c)[0]\n",
    "        if len(idx) == 0: \n",
    "            continue\n",
    "        take = min(per_class, len(idx))\n",
    "        keep_idx.extend(rng.choice(idx, size=take, replace=False))\n",
    "    keep_idx = np.array(keep_idx)\n",
    "    return X[keep_idx], y[keep_idx]\n",
    "\n",
    "# after loading NPZs:\n",
    "Xtr, ytr = stratified_cap(Xtr, ytr, per_class=400, seed=0)\n",
    "Xva, yva = stratified_cap(Xva, yva, per_class=100, seed=1)\n",
    "Xte, yte = stratified_cap(Xte, yte, per_class=100, seed=2)\n",
    "\n",
    "# sanity: show class counts\n",
    "for name, yv in [(\"train\", ytr), (\"val\", yva), (\"test\", yte)]:\n",
    "    vals, cnts = np.unique(yv, return_counts=True)\n",
    "    print(name, \"classes:\", len(vals), \"min/max per class:\", cnts.min(), cnts.max())\n",
    "    \n",
    "\n",
    "print(\"Train:\", Xtr.shape, \"Val:\", Xva.shape, \"Test:\", Xte.shape, \"classes:\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe21168-ac76-4de5-ba77-7d27a1c53fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (N, T, C) -> (N, C, T) once up front\n",
    "Xtr_cf = np.transpose(Xtr, (0, 2, 1)).astype(np.float32, copy=False)\n",
    "Xva_cf = np.transpose(Xva, (0, 2, 1)).astype(np.float32, copy=False)\n",
    "Xte_cf = np.transpose(Xte, (0, 2, 1)).astype(np.float32, copy=False)\n",
    "\n",
    "class EMGCFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_ct, y):\n",
    "        self.X = X_ct\n",
    "        self.y = y.astype(np.int64)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return torch.from_numpy(self.X[i]), torch.tensor(self.y[i], dtype=torch.long)\n",
    "\n",
    "train_ds = EMGCFDataset(Xtr_cf, ytr)\n",
    "val_ds   = EMGCFDataset(Xva_cf, yva)\n",
    "test_ds  = EMGCFDataset(Xte_cf, yte)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True,  num_workers=0)\n",
    "val_loader   = torch.utils.data.DataLoader(val_ds,   batch_size=128, shuffle=False, num_workers=0)\n",
    "test_loader  = torch.utils.data.DataLoader(test_ds,  batch_size=128, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf5e3165-1c3b-4f6e-8120-4e8eec3ff181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def make_norm(norm_type: str, num_channels: int):\n",
    "    \"\"\"Factory for 1D normalization layers.\"\"\"\n",
    "    if norm_type.lower() == \"bn\":\n",
    "        return nn.BatchNorm1d(num_channels)\n",
    "    if norm_type.lower() == \"gn\":\n",
    "        groups = max(1, min(8, num_channels))\n",
    "        return nn.GroupNorm(groups, num_channels)\n",
    "    raise ValueError(\"norm_type must be 'bn' or 'gn'\")\n",
    "\n",
    "class ConvBlock1D(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=5, norm=\"gn\", p_drop=0.1):\n",
    "        super().__init__()\n",
    "        pad = k // 2\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(c_in, c_out, kernel_size=k, padding=pad),\n",
    "            make_norm(norm, c_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(c_out, c_out, kernel_size=k, padding=pad),\n",
    "            make_norm(norm, c_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p_drop),\n",
    "        )\n",
    "    def forward(self, x):  # (B, C, T)\n",
    "        return self.block(x)\n",
    "\n",
    "class CNNLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible CNNâ†’LSTM model for EMG windows.\n",
    "    Input:  (B, C, T)  (channel-first)\n",
    "    Output: logits (B, n_classes)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels: int,\n",
    "        n_classes: int,\n",
    "        conv_channels=(64, 128),      # tuple per block (len = n_conv_blocks)\n",
    "        conv_kernels=(5, 5),          # kernel per block (same length as conv_channels)\n",
    "        pool_stride=2,                # temporal downsampling after each block\n",
    "        norm=\"gn\",                    # \"bn\" or \"gn\"\n",
    "        conv_dropout=0.1,\n",
    "        lstm_hidden=128,\n",
    "        lstm_layers=2,\n",
    "        bidir=True,\n",
    "        lstm_dropout=0.3,\n",
    "        head_hidden=256,\n",
    "        head_dropout=0.3,\n",
    "        temporal_pool=\"mean\",         # \"mean\", \"max\", or \"last\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(conv_channels) == len(conv_kernels), \"conv_channels and conv_kernels must match length\"\n",
    "        self.temporal_pool = temporal_pool\n",
    "        self.pool = nn.MaxPool1d(kernel_size=pool_stride) if pool_stride > 1 else nn.Identity()\n",
    "\n",
    "        # --- CNN stack ---\n",
    "        c_prev = n_channels\n",
    "        conv_blocks = []\n",
    "        for c_out, k in zip(conv_channels, conv_kernels):\n",
    "            conv_blocks.append(ConvBlock1D(c_prev, c_out, k=k, norm=norm, p_drop=conv_dropout))\n",
    "            conv_blocks.append(self.pool)   # downsample T each block\n",
    "            c_prev = c_out\n",
    "        self.cnn = nn.Sequential(*conv_blocks)\n",
    "\n",
    "        # --- LSTM over time ---\n",
    "        # After CNN, we permute to (B, T', C') for LSTM:\n",
    "        self.bidir = bidir\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=c_prev, hidden_size=lstm_hidden, num_layers=lstm_layers,\n",
    "            batch_first=True, dropout=(lstm_dropout if lstm_layers > 1 else 0.0),\n",
    "            bidirectional=bidir\n",
    "        )\n",
    "        lstm_out_dim = lstm_hidden * (2 if bidir else 1)\n",
    "\n",
    "        # --- Head ---\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(lstm_out_dim),\n",
    "            nn.Linear(lstm_out_dim, head_hidden), nn.ReLU(),\n",
    "            nn.Dropout(head_dropout),\n",
    "            nn.Linear(head_hidden, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B, C, T)\n",
    "        h = self.cnn(x)                   # (B, C', T')\n",
    "        h = h.transpose(1, 2)             # (B, T', C') for LSTM\n",
    "        h, _ = self.lstm(h)               # (B, T', H*dirs)\n",
    "\n",
    "        if self.temporal_pool == \"mean\":\n",
    "            h = h.mean(dim=1)             # (B, H*dirs)\n",
    "        elif self.temporal_pool == \"max\":\n",
    "            h, _ = h.max(dim=1)\n",
    "        elif self.temporal_pool == \"last\":\n",
    "            h = h[:, -1, :]\n",
    "        else:\n",
    "            raise ValueError(\"temporal_pool must be one of {'mean','max','last'}\")\n",
    "\n",
    "        return self.head(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace14398-3124-4bd1-82ae-adb6bdbd0590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "690199"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_channels = Xtr.shape[2]    # from your NPZ (N, T, C)\n",
    "n_classes  = len(idx_to_label)\n",
    "\n",
    "# Light CPU-friendly defaults: 2 conv blocks â†’ 2-layer BiLSTM\n",
    "model = CNNLSTM(\n",
    "    n_channels=n_channels,\n",
    "    n_classes=n_classes,\n",
    "    conv_channels=(64, ),\n",
    "    conv_kernels=(5, ),\n",
    "    pool_stride=2,\n",
    "    norm=\"gn\",                # GroupNorm more stable on small batches\n",
    "    conv_dropout=0.1,\n",
    "    lstm_hidden=128,\n",
    "    lstm_layers=2,\n",
    "    bidir=True,\n",
    "    lstm_dropout=0.3,\n",
    "    head_hidden=256,\n",
    "    head_dropout=0.3,\n",
    "    temporal_pool=\"mean\",\n",
    ").to(DEVICE)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a24f900-3874-4800-a980-9d361ed85762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights from training distribution\n",
    "vals, cnts = np.unique(ytr, return_counts=True)\n",
    "class_count = np.zeros(n_classes, dtype=np.float32)\n",
    "class_count[vals] = cnts\n",
    "weights = 1.0 / (class_count + 1e-6)\n",
    "weights = torch.tensor(weights / weights.mean(), dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)  # or nn.CrossEntropyLoss() without weights\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "def run_epoch(model, loader, optimizer=None):\n",
    "    train_mode = optimizer is not None\n",
    "    model.train(train_mode)\n",
    "    total_loss, y_true, y_pred = 0.0, [], []\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        y_true.append(yb.detach().cpu().numpy())\n",
    "        y_pred.append(logits.argmax(dim=1).detach().cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return total_loss / len(loader.dataset), acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec3e4b-96f1-4ef3-aab5-9e824c9caa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train: loss 2.9477 acc 0.117 f1 0.101 | val: loss 2.8388 acc 0.140 f1 0.092\n",
      "Epoch 02 | train: loss 2.8020 acc 0.165 f1 0.146 | val: loss 2.8031 acc 0.170 f1 0.133\n",
      "Epoch 03 | train: loss 2.7369 acc 0.183 f1 0.165 | val: loss 2.8106 acc 0.182 f1 0.146\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "PATIENCE = 5\n",
    "best_val_f1, best_state = -1.0, None\n",
    "no_improve = 0\n",
    "hist = []\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc, tr_f1 = run_epoch(model, train_loader, optimizer)\n",
    "    va_loss, va_acc, va_f1 = run_epoch(model, val_loader,   optimizer=None)\n",
    "    hist.append((epoch, tr_loss, tr_acc, tr_f1, va_loss, va_acc, va_f1))\n",
    "    print(f\"Epoch {epoch:02d} | train: loss {tr_loss:.4f} acc {tr_acc:.3f} f1 {tr_f1:.3f} | \"\n",
    "          f\"val: loss {va_loss:.4f} acc {va_acc:.3f} f1 {va_f1:.3f}\")\n",
    "\n",
    "    if va_f1 > best_val_f1:\n",
    "        best_val_f1 = va_f1\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacfe15-8de7-4dc8-901e-4181c18f998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_loss, te_acc, te_f1 = run_epoch(model, test_loader, optimizer=None)\n",
    "print(f\"Test â€” loss {te_loss:.4f}  acc {te_acc:.3f}  macro-F1 {te_f1:.3f}\")\n",
    "\n",
    "# Full report\n",
    "y_true_all, y_pred_all = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        logits = model(xb.to(DEVICE)).cpu().numpy()\n",
    "        y_pred_all.append(np.argmax(logits, axis=1))\n",
    "        y_true_all.append(yb.numpy())\n",
    "y_true_all = np.concatenate(y_true_all)\n",
    "y_pred_all = np.concatenate(y_pred_all)\n",
    "\n",
    "print(classification_report(y_true_all, y_pred_all, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23e421-427c-4063-99ea-e38ced19883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "ax.set_title(\"Confusion Matrix â€” BiLSTM\")\n",
    "ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = REPORTS_DIR / \"cm_lstm.png\"\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved:\", fig_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1f58d-727d-46c2-8682-c84dedb14dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = MODELS_DIR / \"lstm_db1.pt\"\n",
    "meta_path = MODELS_DIR / \"lstm_db1_meta.npz\"\n",
    "\n",
    "torch.save(model.state_dict(), ckpt_path)\n",
    "np.savez_compressed(meta_path, n_features=n_features, n_classes=n_classes)\n",
    "\n",
    "print(\"Saved model to:\", ckpt_path)\n",
    "print(\"Saved meta   to:\", meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1b2d1-4969-4736-ad27-9582d0a9391b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e360b98-1b0d-4123-a77c-28ad6c864d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
